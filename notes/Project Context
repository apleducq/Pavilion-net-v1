## **Introduction**

**Pavilion** is a proposed B2B “trust broker” hub that facilitates secure and privacy-preserving verification of user credentials between **Relying Parties (RPs)** and **Data Providers (DPs)**. Instead of each RP integrating separately with each DP, Pavilion provides a one-stop integration hub. It enables RPs to verify user **eligibility or credentials** with DPs by using **Verifiable Credentials (VCs)** and advanced privacy techniques (e.g. **privacy-preserving record linkage** and **zero-knowledge proofs**) to minimize data exposure. Pavilion orchestrates requests, enforces policies, and generates cryptographic proofs as needed, all while **minimizing data centralization** – the hub does *not* hoard personal data but acts as a conduit that logs transactions in a **tamper-evident audit log** for accountability. The envisioned tech stack is modern and “cloud-native,” including containerized Go microservices orchestrated by Kubernetes (with a **service mesh** for mutual TLS), **Open Policy Agent (OPA)** for externalized policy decisions, and an identity layer via **Keycloak/ORY** for OAuth2/OpenID Connect (OIDC) support. Data storage is split across fit-for-purpose solutions: Postgres for configurations and contracts, DynamoDB/Firestore for time-limited caches, Redis for hot caching, S3 for bulk data ingestion, and a ledger like AWS Aurora for the audit log. Cryptography libraries (for **PSI/OPRF** protocols, **BBS+ signatures** for selective disclosure, and accumulators for revocation) will provide the privacy-preserving and verifiable credential capabilities. An **observability** stack using OpenTelemetry instrumentation, Prometheus/Grafana for metrics, and the ELK stack for logs is planned to ensure the system can be monitored and debugged effectively.

Building Pavilion from scratch involves **two major phases**: (1) creating and demonstrating a functional **prototype (MVP)**, and (2) a subsequent **build-out and scaling phase** that hardens the system for production, adding long-term features and ensuring compliance with regulations across different regions. Below we outline what each phase entails in detail.

## **Phase 1: Prototype – Building and Presenting the MVP**

### **Objectives of the Prototype**

The goal of the prototype is to deliver a **working proof-of-concept** of the Pavilion trust broker that stakeholders can interact with and evaluate. It should demonstrate the **core use case**: an RP querying a DP (via the Pavilion hub) to verify a user’s credential or attribute, using privacy-preserving methods and logging the event. The prototype isn’t meant to be fully scalable or compliance-certified; rather, it should **prove the concept** and clarify the system’s value. Key objectives include:

- **Basic end-to-end credential verification flow:** Show that an RP can request some verification (e.g. “Is user X eligible for service Y?” or “Provide credential Z for user X”) and receive a trustworthy response via the hub from a DP. This will likely involve issuing or retrieving a **Verifiable Credential** from the DP and presenting it (or a proof of it) to the RP.
- **Privacy protection in action:** Even in the MVP, demonstrate privacy-preserving features in a simple form. For example, use **privacy-preserving record linkage (PPRL)** to match user records between RP and DP without exposing raw personal data. (PPRL can be simulated by hashing or encoding an identifier; in practice, PPRL often uses cryptographic techniques like Bloom filters that allow linking records from different providers **without exposing personally identifiable data**[thoughtworks.com](https://www.thoughtworks.com/radar/techniques/privacy-preserving-record-linkage-pprl-using-bloom-filter#:~:text=Linking%20records%20from%20different%20data,records%20can%20be%20linked%20by).) If applicable, show a **zero-knowledge proof** or selective disclosure – e.g. proving a user meets a criterion **without revealing the underlying data**[en.wikipedia.org](https://en.wikipedia.org/wiki/Zero-knowledge_proof#:~:text=Proving%20validity%20without%20revealing%20other,data). This could be as simple as proving age > 18 without disclosing the birthdate, using a ZK proof or a VC with selective disclosure.
- **Policy-based access control:** Use OPA to enforce a sample policy during the flow. For instance, a policy might state that *only certain RPs can request certain data from certain DPs*, or that *user consent must be present*. OPA’s role in the MVP is to make such decisions external to the code. (OPA is a general-purpose policy engine allowing policy-as-code and unified enforcement across microservices[openpolicyagent.org](https://www.openpolicyagent.org/docs#:~:text=The%20Open%20Policy%20Agent%20,pipelines%2C%20API%20gateways%2C%20and%20more). In Pavilion, when the hub gets a verification request it can query OPA – with input context like RP identity, requested credential type, etc. – to decide if the request is allowed under configured policies[openpolicyagent.org](https://www.openpolicyagent.org/docs#:~:text=OPA%20decouples%20policy%20decision,arbitrary%20structured%20data%20as%20input).)
- **Tamper-evident logging:** The prototype should log each transaction (or at least the critical ones) in an append-only audit log to illustrate the **tamper-evidence** concept. This could be a simplified local version of the audit ledger – e.g. using a local instance of **Trillian** (an open-source verifiable log). Trillian can maintain an immutable, verifiable history of events (similar to a blockchain, it’s an append-only Merkle tree log[transparency.dev](https://transparency.dev/#:~:text=Trillian%20is%20a%20log%20that,ledger%20based%20ecosystems%2C%20certificate%20transparency)). In the MVP, each credential request/response could be logged with a hash, timestamp, and actors, and the prototype can show that the log detects any tampering. (If running Trillian is too heavy for a demo, even logging to a file with digital signatures could simulate the idea.)

### **Prototype Architecture and Components**

For ease of development and demonstration, the prototype can be set up in a **local containerized environment** – for example, using Docker Compose on a single machine (your PC). This avoids the complexity of a full multi-cloud K8s deployment, while mimicking the microservice architecture. The key components to deploy in the MVP architecture include:

- **Pavilion Hub Service (Core Orchestrator):** A Go-based service that implements the main logic of the trust broker. It exposes an API endpoint for RPs to make verification requests. Internally, it will forward queries to the appropriate DP (or multiple DPs), handle the responses/credentials, apply policies, and return a result to the RP. This service will also interface with the audit log (writing each transaction entry) and with OPA for policy decisions. In the prototype, this might be a single container running the Go service. Security features like mTLS between services can be initially simplified (e.g. using HTTP within Docker network, or basic TLS) just to focus on functionality. However, the design should keep in mind that in a real deployment, each service-to-service call will be secured by the service mesh with mutual TLS.
- **Relying Party Simulator:** For the demo, you’ll need a way to simulate an RP client. This could be a simple web frontend or CLI script that an operator can use to initiate a verification. For example, a small web app where one can enter “UserID X, Request Y” which then calls the Pavilion API. The RP simulator would obtain an OAuth/OIDC token from Keycloak to authenticate to the hub (simulating that RPs must be registered/authorized). This component shows the RP side of the interaction. It might be simplest as a small script or app triggered during the demo rather than a fully interactive GUI.
- **Data Provider Service (Simulator):** At least one DP needs to be represented to complete the flow. You can create a dummy DP service that holds some sample user data or credentials (for instance, a mini database of users and an attribute like “isStudent=true” or some credential claims). The DP service should have an API that the Pavilion hub calls to request verification for a user. In a real system, the DP would likely issue a Verifiable Credential (VC) or provide a proof. In the prototype, you can simulate this by the DP returning a signed JSON (to mimic a VC) or just a boolean answer with a simple signature. If you want a more realistic approach, consider using a **VC library** (there are libraries for W3C VC in various languages) to have the DP issue an actual verifiable credential that the RP (or Pavilion on its behalf) can verify. But keeping it simple at first – maybe the DP service returns a pre-made credential or a claim.
- **Identity & Auth (Keycloak/ORY):** Incorporate an *Identity Provider* component to manage OAuth2/OIDC, as indicated in the stack. **Keycloak** is a good choice for the prototype because it’s open-source and relatively easy to containerize. In the MVP, Keycloak can handle two aspects:
    1. **RP Authentication** – RPs (or the demo client acting as an RP) can authenticate and obtain a token to call Pavilion’s API. Pavilion will validate this token to identify the caller.
    2. **User Authentication/Consent** – If your flow involves end-user consent (for data sharing from DP), Keycloak could serve as an OIDC provider for user login and consent screens. For example, Pavilion might redirect the user to Keycloak to login and authorize DP to release a credential to the RP. This might be complex to implement fully in a short MVP, so it could be simplified (e.g., assume consent given, or use static tokens).
        
        Keycloak provides these features out-of-the-box so you don’t have to write them yourself[keycloak.org](https://www.keycloak.org/docs/latest/server_admin/index.html#:~:text=Edit%20this%20section%20Report%20an,issue). You can run Keycloak in a container and pre-configure a realm with a test RP client and some demo user accounts as needed.
        
- **Policy Engine (OPA):** Run OPA as a sidecar or a standalone container that Pavilion can query. In prototype, you could embed simple policies directly in Pavilion’s code for expedience, but a more “real” approach (and to stay aligned with final architecture) is to have an OPA container loaded with a basic policy (written in Rego) for access control. Pavilion’s service calls OPA’s REST API (e.g. **`POST /v1/data/pavilion/policy`** with input) to get an allow/deny decision or other decisions. For example, a policy might check that “RP X is allowed to request credential Y from DP Z” and that the user has given consent. The OPA policy can be as simple as a whitelist of allowed RP-DP-data combinations in the MVP. By including OPA now, you demonstrate the decoupling of policy decisions from code[openpolicyagent.org](https://www.openpolicyagent.org/docs#:~:text=OPA%20decouples%20policy%20decision,arbitrary%20structured%20data%20as%20input), which will be crucial later for flexibility.
- **Audit Log (Ledger):** Set up a rudimentary tamper-evident log. If you opt for **Trillian**, it can be run with a couple of components (a log server and a MySQL storage for leaves). However, Trillian setup might be overkill for a quick demo. An alternative is to simulate an append-only ledger by using a database table or file where each entry includes a cryptographic hash linking to the previous entry. For example, you can implement a simple chain: each log entry stores **`hash = H(prev_hash + current_event)`**. The “tamper-evident” property can be shown by demonstrating that modifying an earlier entry breaks the chain. If possible, show that the system can produce a proof of inclusion for an entry (even if it’s just recomputing hashes). For simplicity, you might just log to console or a file during the demo but explain that *in production this would be a verifiable ledger on a public blockchain* which ensures an immutable history [aws.amazon.com](https://aws.amazon.com/qldb/#:~:text=Maintain%20an%20immutable%2C%20cryptographically%20verifiable,log%20of%20data%20changes). The main point to get across is that every request and response is recorded in a way that any alteration would be detectable, thus providing an audit trail for compliance and trust.
- **Temporary Data Stores:** The prototype likely doesn’t need all the cache layers mentioned (Redis, Dynamo/Firestore, etc.), because the data volume is small. You could use an in-memory cache in the code or a single Redis container for all caching to demonstrate how hot data might be stored. For example, if the DP issues a credential, Pavilion might cache it briefly (with a TTL) so if the RP asks again it doesn’t repeat the whole process. Having Redis in the MVP to simulate a “hot cache” is optional but could be included to show that architecture piece working (for instance, caching recently verified results or public keys of issuers, etc.). Postgres can be included to store static configuration like which DPs and RPs are onboarded and their public keys or endpoints (essentially simulating a registry of parties and contracts). These could alternatively be just config files for the prototype, but a lightweight Postgres container with a simple schema (tables for RPs, DPs, maybe a table for issued credentials if needed) will make it feel more realistic.

All these components can be defined in a **Docker Compose** YAML so that with one command you spin up the whole environment. The network configuration will allow the Pavilion service to talk to OPA, Keycloak, DP, etc., as if they are separate microservices. You can use a reverse proxy or just direct container ports for communication. For example, Pavilion might call **`http://opa:8181/v1/data/pavilion/policy`** in the compose network to query OPA, Keycloak might be at **`http://keycloak:8080`**, etc. Using containers ensures that the prototype is **self-contained**, easily shareable, and similar to how the production would be containerized (just not yet scaled or orchestrated by Kubernetes at this stage).

### **Prototype Demonstration Scenario**

In the MVP demo, you would walk through a scenario that ties everything together. For instance, consider the use-case of **university enrollment verification**:

- A user (let’s call her Alice) is trying to sign up for a student discount at an online service (the RP). The RP needs to verify Alice is indeed enrolled at a university (the DP in this case could be an education credentials database or university API).
- Alice (the user) or the RP triggers a verification request via the Pavilion hub: *“Is Alice a current student?”*.
- Pavilion receives this request via its API. It authenticates the RP’s request (using the OAuth token to ensure the RP is who it claims). Pavilion then consults OPA to ensure policy allows this query (e.g., RP is authorized to ask for enrollment status, and maybe Alice has given consent). Suppose policy passes.
- Pavilion then performs privacy-preserving record linkage to locate Alice’s record at the DP without exposing Alice’s full identity. For example, Pavilion might send a **hashed identifier** or a Bloom-filter encoded name/DOB to the DP’s service. (PPRL using Bloom filters allows two parties to find matches between their records **without revealing the actual personal data**, by comparing the probabilistic encodings[thoughtworks.com](https://www.thoughtworks.com/radar/techniques/privacy-preserving-record-linkage-pprl-using-bloom-filter#:~:text=Linking%20records%20from%20different%20data,records%20can%20be%20linked%20by).) In the prototype, this could simply be: Pavilion sends Alice’s email hashed to the DP; the DP service checks its database for the same hash to find Alice’s record.
- Upon finding a match, the DP service either **issues a Verifiable Credential** asserting Alice’s student status, or sends back a signed assertion. For the demo, you might have the DP respond with a JSON Web Token or JSON-LD credential saying “Alice is enrolled = true” signed by the DP’s key. If using an actual VC format, it would include claims about Alice (perhaps just an ID or name and an attribute “enrollmentStatus: active”) and a proof section (signature).
- Pavilion receives this credential/response. It verifies the signature (ensuring it indeed came from the trusted DP). Then Pavilion could perform a **selective disclosure** or proof derivation if needed – for instance, if the DP returned a full credential with many attributes, Pavilion (or Alice’s wallet in a real SSI scenario) could strip it down to only the needed claim. In our simple case, the only claim is enrollment status, so nothing else to minimize. (If demonstrating BBS+ selective disclosure, one could show a credential with multiple fields, then use BBS+ to present only one field with a proof that it’s valid[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=strong%20privacy%20preserving%20security%20properties%3B,of%20knowledge%20and%20selective%20disclosure). However, implementing BBS+ fully in an MVP is advanced; you might just conceptually explain it or use a library if available. BBS+ basically allows the holder of a credential to prove certain signed attributes without revealing others, ensuring **data minimization and unlinkability** across presentations[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=The%20combined%20approach%20of%20W3C,preserving%20credential%20or%20anonymous%20credential)[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=strong%20privacy%20preserving%20security%20properties%3B,of%20knowledge%20and%20selective%20disclosure).)
- Finally, Pavilion returns a response to the RP. If using OIDC, this could be via an ID token or UserInfo response containing the verified claim. In a simpler REST approach, it could just be an API response “Verified: Yes, Alice is a student (per DP X)”. The RP thus receives high-confidence, audit-trailed evidence of Alice’s status without having had to integrate directly with the university’s system.

During this flow, the **audit log** is append-only recording each step: RP’s request, DP’s response (or issuance of credential), etc., each with hashes and signatures to later verify none has been altered. You can demonstrate querying the audit log – e.g., show an entry that corresponds to Alice’s verification, and perhaps use a simple script to validate the chain of hashes proving integrity.

From a user perspective, you (as the demonstrator) might simulate Alice’s consent by pre-approving in Keycloak or by having a consent flag set. In a live system, Alice could authenticate and consent via an OAuth flow, but for MVP it’s okay to stub this out.

The **prototype presentation** would involve showing the components working together. For example:

- In a terminal or browser, trigger the RP’s verification request. Perhaps log into the RP demo interface (if any auth needed), then initiate the check.
- Switch to logs or a dashboard to show Pavilion receiving the request (maybe you have console logs from the Go service indicating what’s happening).
- Show that OPA was queried (could show OPA logs or a decision result).
- Show the DP service log that it received a request for Alice and issued a response. Possibly show the content of the credential/response (to highlight that, for instance, it did not include unnecessary personal info).
- Then show Pavilion’s final output to the RP (e.g., a success message).
- Additionally, open the audit log (could be as simple as viewing a file or database table) to show an entry for this transaction, and demonstrate how the hash chain works to detect tampering.

This end-to-end demo on your PC proves the concept’s feasibility. It also gives you a chance to collect feedback – maybe the stakeholders will suggest additional features or note certain compliance requirements at this stage, which leads into the next phase.

**Note:** Performance and scalability are not concerns in the prototype – you can have artificial delays or not worry about optimizing cryptographic operations. The emphasis is on **functional correctness and privacy/security properties** being visible.

### **Tools and Shortcuts in the MVP**

During prototyping, it’s acceptable to use some **shortcuts or simulated components** for simplicity, as long as the overall architecture is represented. For example, if configuring a full service mesh (Istio/Linkerd) is too heavy for now, you can simply ensure all inter-service calls use HTTP over **`localhost`** (inside Docker network) and perhaps demonstrate *one* mTLS connection (say between Pavilion and DP) using self-signed certs to illustrate encryption. The full zero-trust service mesh with automatic mTLS can come in Phase 2.

Similarly, for cryptography: you might not implement the most complex ZKPs in the MVP by hand. Instead, use existing libraries or static examples. There are open-source implementations of **PSI** and **OPRF** (Oblivious Pseudo-Random Function) that could be tried if you want to show, for instance, a private set intersection between two sets. But an easier approach is to demonstrate the *concept* of PSI: e.g., show that Pavilion and DP can each have a set of user email hashes and they can find the common one without revealing the actual emails. A simple Python script using an OPRF-based PSI library could be run offline to illustrate this concept, if needed. (PSI, in essence, “allows two parties to compute the intersection of their datasets without exposing their raw data”[openmined.org](https://openmined.org/blog/private-set-intersection/#:~:text=Private%20set%20intersection%20,as%20a%20location%2C%20ID%2C%20etc) – you can explain this while highlighting that in the real system this happens behind the scenes to match records or membership.)

For **BBS+ selective disclosure**, if you have time, libraries like Hyperledger Aries or Mattr’s BBS+ implementation could be used to produce a toy credential. If not, you can simulate selective disclosure by showing a JSON credential with multiple fields and then showing a derived JSON with one field revealed and a dummy proof. Stakeholders generally understand demos at this stage may have some “wizard of Oz” behind the curtain. Just be transparent about which parts are fully working and which are mocked or simplified.

Finally, ensure **observability in the prototype** to the extent useful for demo. This might mean running a local Prometheus and Grafana to visualize system metrics (CPU, memory, request count) and perhaps a simple custom metric like “# of verifications” in Pavilion. OpenTelemetry instrumentation can be added to the Go services to emit traces or metrics. For example, Pavilion could be instrumented to trace the flow (RP call → DP call → response) which you could then view in a local Jaeger UI, demonstrating distributed tracing. This is not strictly required for a functional demo, but since the observability stack is part of the plan, it impresses stakeholders if you show a Grafana dashboard with live metrics from the demo. (OpenTelemetry can be configured to export metrics to Prometheus and traces to Jaeger easily in a local setup. Integrating it with Prom/Grafana/Jaeger provides a comprehensive view of the system’s behavior even at this prototype stage[medium.com](https://medium.com/cloud-native-daily/harnessing-opentelemetry-with-jaeger-elk-and-prometheus-e06360de831a#:~:text=Harnessing%20OpenTelemetry%20with%20Jaeger%2C%20ELK%2C,Observability%20solution%20for%20your%20applications).)

**In summary,** the prototype phase produces a mini-version of Pavilion running on your PC, with multiple Docker containers emulating the various microservices. It demonstrates the core flow of credential verification with privacy preservation, uses real components like OPA for policy and Keycloak for auth where feasible, and logs actions immutably. This MVP will serve as a foundation to identify any gaps and will guide the design for the full-scale implementation.

## **Phase 2: Build-Out and Scaling for Production**

Once the prototype has proven the concept, the next phase is to **evolve Pavilion into a production-grade service**. This involves enhancing the architecture for scalability, reliability, and security, as well as integrating additional features needed for real-world deployment. A crucial aspect is also **meeting compliance requirements across different regions and industries**, which influences data handling, privacy measures, and operational processes. Below we outline the key considerations and tasks in this build-out phase.

### **Scaling the Architecture**

In production, Pavilion will likely be deployed on a **Kubernetes cluster** (or multiple clusters) to manage the microservices and allow horizontal scaling. Each component from the prototype becomes a deployable service with redundancy and auto-scaling as needed:

- **Go Microservices & Kubernetes:** The Pavilion core service, and any other internal services (like perhaps separate services for credential issuance, verification, etc.), will run as deployments in K8s. They can be scaled out (multiple replicas) to handle increasing load from many RPs and DPs. Kubernetes will also help with rolling updates and self-healing. We will introduce a **service mesh** (e.g., Istio, Linkerd, or Consul) which will inject sidecar proxies to all pods. This mesh will enforce **mutual TLS (mTLS)** by default for all service-to-service calls, fulfilling the zero-trust networking principle. With mTLS, every microservice call is encrypted and identities of services are verified using certificates – this prevents man-in-the-middle attacks and eavesdropping within the cluster. The mesh also adds traffic management capabilities (for instance, if we later have a global deployment, the mesh can route requests to the nearest regional service, etc.). The overhead introduced by the mesh is acceptable for a B2B system, and it greatly simplifies securing communications (one YAML to enforce mesh-wide mTLS[cloud.google.com](https://cloud.google.com/service-mesh/docs/tutorials/mtls#:~:text=Cloud%20Service%20Mesh%20by%20example%3A,applying%20a%20single%20YAML%20file)).
- **High Availability and Failover:** In scaling up, ensure there are no single points of failure. For example, run multiple replicas of the Pavilion hub in active-active mode behind a Kubernetes Service (or ingress). If one instance fails, others continue serving. The databases (Postgres, etc.) in production should be set up in HA configurations as well (e.g., Postgres primary/replica or use a cloud managed Postgres with failover). Caches like Redis might be deployed in a clustered mode or with a replica for failover. The audit log (if using QLDB) is inherently multi-AZ in AWS; if using Trillian on self-managed servers, run redundant instances and regularly back up the log storage. Also plan for **disaster recovery**: regular backups of Postgres and any stateful stores, and possibly a multi-region active-active or active-passive setup (discussed more under compliance).
- **Performance Optimization:** The production system must handle potentially a high volume of requests from many RPs. That means optimizing each layer:
    - Use **Redis caching** aggressively for hot data. For instance, cache the results of recent verifications for a short time to avoid hitting DPs repeatedly (TTL caches as planned). Cache also static reference data like DP public keys, schema of credentials, etc. A cache hit should allow Pavilion to reply instantly for repeated queries if policy permits.
    - Use DynamoDB/Firestore (or another fast key-value store) for distributed caching or as a *lazy loading* cache that persists across restarts. The design mentions DynamoDB/Firestore for TTL caches – a practical usage might be caching linkages or tokens that are expensive to recompute. In production, hooking up these services (if in cloud) will give globally distributed low-latency caches (e.g., Firestore for GCP, Dynamo for AWS).
    - The Go services should be instrumented for efficiency: use goroutines to handle concurrent calls (e.g., if a query needs to go to multiple DPs in parallel). Also consider using message queues if needed – for example, if some verifications can be done asynchronously, a queue (like Kafka or cloud pub/sub) could buffer bursts of requests. Given the interactive nature of many credential verifications, synchronous HTTP will be the norm, but for bulk processing (say an RP wants to verify 1000 users at once), an async pipeline might be better.
    - Ensure the cryptographic operations are using efficient libraries (e.g., hardware acceleration where possible). If using BBS+ signatures and zero-knowledge proofs, those can be CPU intensive; consider using native implementations in Rust or C called from Go for heavy crypto if needed, or even offloading some tasks to specialized services/functions.
- **Modularizing Services:** As the system grows, you might split the monolithic Pavilion core into multiple microservices for clarity and scalability. For example, a dedicated **“Credential Issuer/Verifier” service** that handles all VC-related cryptography, separate from the **“Orchestration & Policy” service** that handles routing and policy enforcement. Similarly, a **“User Identity/Consent” service** might appear (though Keycloak covers a lot, you may write custom logic to integrate user consent flows if needed). Decoupling services allows independent scaling – e.g., if cryptographic proof generation is the bottleneck, you can scale out just that service on more powerful instances.
- **Monitoring and Observability:** In production, robust observability is crucial. All services will have **OpenTelemetry** instrumentation enabled so that traces, metrics, and logs can be collected. A centralized tracing system like Jaeger or Grafana Tempo will collect distributed traces – this helps diagnose issues in multi-step transactions (RP -> Pavilion -> DP etc.). **Prometheus** will scrape metrics from services (like request counts, latency, error rates, cache hits, etc.), and **Grafana** will be used to visualize these metrics on dashboards. For logs, using **ELK (Elasticsearch, Logstash, Kibana)** or a cloud log service will enable searching through audit and error logs. You might also integrate alerting: e.g., set up alerts for high error rates, unusual latency (could indicate a DP timeout), or audit log inconsistencies. The observability stack essentially stays the same as envisioned, but in production we ensure it’s properly tuned (e.g., not too much overhead from tracing sampling, secure access to logs, retention policies for logs to meet compliance, etc.). By having metrics and logs from day one, the operations team can ensure reliability and quickly troubleshoot issues. (The integration of OpenTelemetry with Prometheus/Grafana and ELK was partially shown in the prototype; in production it will be fully realized, giving a comprehensive monitoring solution[medium.com](https://medium.com/cloud-native-daily/harnessing-opentelemetry-with-jaeger-elk-and-prometheus-e06360de831a#:~:text=Harnessing%20OpenTelemetry%20with%20Jaeger%2C%20ELK%2C,Observability%20solution%20for%20your%20applications).)

### **Enhancing Privacy and Cryptographic Features**

The prototype likely had rudimentary implementations of the advanced cryptography. In the build-out phase, these features will be fully implemented and refined, since privacy and security are the selling points of Pavilion:

- **Verifiable Credentials (VCs):** Standardize on the W3C VC Data Model for any credentials exchanged. Pavilion should handle credentials in an interoperable format (JSON-LD or JWT-based VCs). This means integrating libraries or services that can create, sign, and verify VCs. For instance, if Keycloak supports issuing OIDC Verifiable Credentials (there are emerging standards like OIDC for VC issuance – OID4VCI), Pavilion could leverage that for certain credentials. In other cases, Pavilion might directly use a library like DIDKit or Hyperledger Aries to generate verifiable presentations. The system should be capable of both **issuing** VCs (in cases the DP is not VC-aware, Pavilion might act as an issuer on their behalf by wrapping DP’s data into a VC) and **verifying** presentations from users. Over time, supporting multiple credential formats (LD-Proofs vs JWT, different signature suites like BBS+, CL-signatures, etc.) might be necessary to interoperate with various identity systems. Initially, focusing on one format (say JSON-LD with BBS+ or JWT with SD-JWT) based on partner requirements is wise, but keep an extensible design.
- **Selective Disclosure via BBS+ or SD-JWT:** Implement the chosen selective disclosure mechanism fully. The stack mentions **BBS+** which is a good choice for unlinkable selective disclosure in VCs[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=strong%20privacy%20preserving%20security%20properties%3B,of%20knowledge%20and%20selective%20disclosure). In production, you would integrate a BBS+ crypto library (for BLS12-381 curves likely) in the DP or Pavilion. For example, when a DP issues a credential, it signs it with BBS+ so that later the user (or Pavilion acting as an intermediary) can derive a proof revealing only requested attributes. This requires managing BBS+ keys and possibly including BBS+ public keys in DIDs or some discovery mechanism. An alternative or complementary approach is **Selective Disclosure JWT (SD-JWT)**, which is another emerging standard (though it doesn’t inherently provide unlinkability like BBS+). Since BBS+ is mentioned, presumably the aim is for strong privacy. So, one of the build-out tasks is to **implement BBS+ signature support**: either by using an open source implementation or incorporating a service like MATTR’s tools. The outcome is that Pavilion can request from the user/DP a VC and specify which fields it needs, and the system can produce a proof that reveals only those fields. For example, if an RP just needs age verification, the DP might issue a full ID credential (name, birthdate, address, etc.), but Pavilion (or the user’s wallet) will generate a BBS+ proof containing only “birthdate > 18” as the revealed info, without disclosing name or exact birthdate. This greatly enhances privacy in cross-party data sharing[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=A%20first%20needed%20property%20is,minimization%20feature%20or%20selective%20disclosure)[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=,threshold%2C%20without%20revealing%20the%20balance).
- **Privacy-Preserving Record Linkage (PPRL):** In production, if Pavilion needs to routinely link records (e.g., match users between RP and DP datasets) without a common identifier, robust PPRL techniques should be utilized. The Bloom filter approach to PPRL is well-documented and should be implemented with care to avoid known attacks (there have been cryptanalysis papers on Bloom filter PPRL vulnerabilities, so using updated techniques and possibly salted hashes or secure multiparty computation variants is important). Pavilion could incorporate a library or service that, given personal data from RP and DP, produces similarity scores or matches in a privacy-safe way[thoughtworks.com](https://www.thoughtworks.com/radar/techniques/privacy-preserving-record-linkage-pprl-using-bloom-filter#:~:text=Linking%20records%20from%20different%20data,records%20can%20be%20linked%20by). If the prototype used simple hashing, the production version might use, for example, a combination of phonetic encoding + Bloom filters for names to allow fuzzy matches without exposing raw names. This is especially relevant if Pavilion will operate in scenarios like health or finance where unique IDs might not be shared across organizations. PPRL ensures **data minimization**: only obfuscated data is exchanged to perform the linkage.
- **Zero-Knowledge Proofs (ZKPs):** Where it adds value, integrate ZKP protocols. Not every verification needs an explicit ZKP, but some use-cases do. For instance, perhaps a DP holds an attribute (like a credit score or salary) that the RP shouldn’t see directly, but the RP wants a proof that it exceeds a threshold. This could be achieved via range-proof ZKPs (bulletproofs or zk-SNARKs) or by encoding as VC + selective disclosure. We should evaluate where pure cryptographic ZKPs are needed versus the VC approach:
    - **ZKP for complex statements:** If Pavilion wants to prove a statement that isn’t just revealing a field in a credential (say, proving a combination of data points satisfies a policy), a custom ZK circuit might be needed. For example, proving a user’s medical test results meet certain criteria without revealing the actual results.
    - **Integration:** In build-out, we might integrate a ZKP framework (like ZoKrates, snarkJS, or use a platform like Hyperledger Ursa or Idemix for certain proofs). Another angle is using **accumulators** (also mentioned in the stack for revocation) which themselves often rely on ZK proofs of non-membership.
    
    Pavilion could provide an API for DPs to register ZKP templates or circuits for specific verifications. But this is likely a later feature – initially, focus on built-in ZKP for a few common needs (like age, greater-than proofs). The architecture should remain modular so that adding a new ZKP module (perhaps as a microservice or a function) is possible without affecting the whole system.
    
- **Private Set Intersection (PSI)/OPRF:** Solidify the use of PSI protocols for any set-membership queries. For example, an RP might ask “Is user X in DP’s denied list?” – this is essentially a set intersection of {X} with DP’s large set. Using an **Oblivious PRF** can enable this: Pavilion and DP can run a protocol where Pavilion learns if there’s a match but DP learns nothing about the query (and vice versa)[openmined.org](https://openmined.org/blog/private-set-intersection/#:~:text=Private%20set%20intersection%20,as%20a%20location%2C%20ID%2C%20etc). In production, we’d incorporate a tested PSI library (there are academic implementations and some industrial ones, e.g., Google’s PSI, or Cloudflare’s PIR, etc.). We’d ensure it scales for the sizes needed (maybe tens of thousands of elements). OPRF (Oblivious PRF) can be used to blind the query item so DP doesn’t know which element is being tested, yet DP can apply its secret and return a value that Pavilion can use to check membership. The result: **membership proof** without revealing the item to DP or the whole set to Pavilion. This will be valuable for compliance with data minimization laws: e.g., verifying someone’s certification without revealing the entire list of certified individuals.
- **Revocation and Status Checking:** In a credential system, **revocation** is crucial – a credential that was valid yesterday might be revoked today. The tech stack mentions *optional accumulators for revocation*. Cryptographic accumulators (like RSA-based or elliptic curve accumulators) can be used so that a holder can prove a credential is not revoked without revealing which credential they hold. Pavilion should incorporate a strategy for revocation checking. For instance, if using a Hyperledger Indy style approach, there could be **revocation registries** and ZK proofs of non-revocation. If using accumulators, DPs (as issuers) would publish accumulator values and issuance witness, and Pavilion/holder can update and prove non-revocation. This is complex, but fortunately not all use-cases require privacy-preserving revocation (some might be fine with an online check by ID). However, since privacy is a theme, implementing accumulators would align with keeping user identity hidden even during revocation checks. In the build-out, plan a **Revocation Service** that monitors revocation lists or accumulators from each issuer/DP. Pavilion could query this service whenever a credential is presented to ensure it’s still valid. Over time, this could be automated by subscribers or by holders presenting non-revocation proofs as part of their VC presentation.

In summary, the build-out phase will transform any “mocked” cryptographic function from the MVP into a **fully functional, standardized cryptographic workflow**. We will adhere to industry standards (W3C, DIF, etc.) wherever possible so that Pavilion can interoperate with other systems (for example, if a government ID is available as a VC in a user’s wallet, Pavilion could verify it and not necessarily depend on a DP API). By the end of this phase, Pavilion should provide a **high-assurance trust framework**: RPs can trust the answers they get (due to signatures and proofs), and users/DPs trust that minimal necessary data was disclosed (due to ZKPs, selective disclosure, etc.).

### **Security, Compliance, and Regional Regulations**

Building trust in a system like Pavilion is not only about technology but also about **compliance with legal and regulatory requirements**, which can vary by region. As we scale out, we need to bake compliance considerations into the design:

- **Data Residency and Sovereignty:** A major regional concern is where personal data is stored and processed. Laws such as GDPR in Europe mandate that personal data of EU citizens remain under certain protections and often physically within certain jurisdictions[ibm.com](https://www.ibm.com/think/insights/data-residency-why-is-it-important#:~:text=1,the%20data%20storage%20and%20processing). Other countries have their own data residency laws. Pavilion’s design to *minimize data centralization* inherently helps – it tries to avoid storing sensitive personal data at the hub at all. However, some data will inevitably pass through Pavilion (even if encrypted or transient). To meet regional requirements, we should consider a **multi-region deployment**:
    - Deploy Pavilion instances in different regions (EU, North America, Asia, etc.) and route traffic such that data doesn’t leave its region unnecessarily. For example, if an EU RP is verifying with an EU DP about an EU user, that transaction should be handled by our EU-based servers and audit log, not by a US server. We might achieve this by **geo-routing** (DNS or application logic that directs to the correct regional cluster based on RP/DP origin or user’s residency).
    - Keep audit logs regional as well – i.e., an EU audit ledger for EU transactions, stored in EU data centers, to satisfy local regulations. If a centralized audit view is needed, it could be composed of hashes or pointers so that the actual data stays in region.
    - Ensure that any cross-region data transfer (if it must happen) complies with mechanisms like Standard Contractual Clauses or equivalents. Ideally, design the system so cross-region transfers are minimal or zero. Pavilion can act as a *federated system* where regional hubs cooperate via privacy-preserving means (for example, two hubs could use PSI across regions to match a user without fully sharing data).
    - Cloud providers offer region-specific services, so use those (e.g., deploy in AWS EU-West for Europe, AWS GovCloud if needed for US sensitive data, etc.). The architecture could leverage a **hub-of-hubs** model: a global coordinator for non-sensitive routing, and local hubs for actual data handling.
- **Privacy and Data Protection Compliance:** Pavilion should undergo Privacy Impact Assessments and align with frameworks like GDPR’s principles (data minimization, purpose limitation, user consent, right to audit, etc.). Features to implement:
    - **Consent Management:** Integrate a clear mechanism for user consent. This might mean building a **Consent Portal** where users can see and manage which RPs can access which data via Pavilion. Technically, when an RP requests data, Pavilion should check if the user has a record of consent for that specific exchange (or a broader consent). If not, it should trigger a consent request (which could be an OIDC/OAuth flow where the user approves sharing of “Attribute X from DP Y with RP Z one-time or ongoing”). Logging these consents in the audit trail is important for compliance (so you can show regulators that no data was shared without user permission).
    - **Data Retention Policies:** As a broker, Pavilion might not need to persist personal data long-term, but it will have caches and logs. Implement policies to purge or anonymize data after it’s no longer needed. For example, the TTL caches in Dynamo/Firestore ensure ephemeral data auto-expires. Audit logs might keep hashes but not raw values. Or if they keep some identifiers, perhaps allow them to be hashed after a period. Compliance often requires not keeping data indefinitely, so define retention periods (maybe based on contractual needs or legal requirements – e.g., keep logs for 1 year for audit, then delete or aggregate).
    - **Security Certifications:** To gain trust of enterprises, Pavilion should eventually comply with security standards like **ISO 27001, SOC 2,** etc. Early on, design with those in mind: implement strict access controls, network isolation, encryption of data at rest and in transit, regular security testing, and detailed audit logs (which we have). For example, every admin action on Pavilion (like configuring a new RP/DP connection) should be logged (Keycloak can log admin events[keycloak.org](https://www.keycloak.org/docs/latest/server_admin/index.html#:~:text=,Mitigating%20security%20threats), and we can route that into our audit system too). Additionally, follow best practices for secrets management (use Vault or K8s secrets for keys, ensure keys like the ones used for signing credentials are stored in HSMs or secure modules when in production).
- **Regional Compliance and Standards:** Different regions might have specific standards for digital identity. For instance, in the EU, **eIDAS 2.0** (European Digital Identity Framework) is introducing the concept of European Digital Identity Wallets (EUDIW). Pavilion should aim to be compatible with such ecosystems – e.g., if a user holds a national ID credential, Pavilion’s DP integration should be able to accept that as input. Also, if Pavilion operates in regulated sectors:
    - In finance, there may be KYC/AML regulations. Pavilion could help in KYC by brokering between banks and government data (for example), but then Pavilion itself might need to follow financial data security guidelines and possibly get regulated (depending on jurisdiction).
    - In healthcare, HIPAA (in the US) would mandate certain safeguards if personal health information flows through Pavilion. While Pavilion tries not to store data, being a conduit still means it must ensure confidentiality (strong encryption), and business associate agreements might be needed with healthcare customers.
    - Government procurement might require FedRAMP (for US federal use) or similar. These typically require robust audit logging (which we have), data integrity, and often on-shore data storage. Pavilion’s tamper-evident logs and strong cryptographic integrity proofs could actually be a selling point for compliance (e.g., demonstrating to auditors that records are intact and verifiable[transparency.dev](https://transparency.dev/#:~:text=Simplify%20regulatory%20compliance)[transparency.dev](https://transparency.dev/#:~:text=By%20using%20a%20tamper,they%20haven%27t%20been%20tampered%20with)).
- **Policy Enforcement & Localization:** Use OPA not just for access control, but also for enforcing regional rules. For example, a policy might dictate: *if an RP in EU requests data from a DP in EU about an EU citizen, ensure the processing happens in EU region and no identifying data is sent to outside regions.* This kind of meta-policy can be configured and OPA can evaluate context to comply with data localization laws. OPA can also enforce purpose limitations: e.g., “RP can only use this data for reason X and only store for Y days” – while OPA can’t enforce what RP does after, Pavilion could refuse requests that don’t meet policy, and at least log intended purpose. Over time, these policies can be refined as laws evolve. The advantage of externalizing them (with OPA) is that new compliance requirements can often be addressed by writing new rules rather than changing code.
- **Audit and Transparency:** Pavilion’s tamper-evident audit log will be extremely useful for demonstrating compliance. Auditors or regulators (with proper permission) should be able to examine the audit trail to see *who accessed what, when, and under which policy*. The use of a verifiable log (like Trillian) means Pavilion can **cryptographically prove the integrity** of its audit records[transparency.dev](https://transparency.dev/#:~:text=Trillian%20provides%20support%20for%20several,tree%2C%20Trillian%20can%20cryptographically%20prove). For instance, if a regulator asks “show me that no unauthorized party accessed data last year,” Pavilion can produce a consistency proof of the log from previous checkpoints. This level of transparency is rare and will bolster trust. As part of scaling, set up a process to **monitor the audit log** for anomalies – e.g., an unexpected spike in certain queries could signal misuse. Multiple parties (RPs, DPs or independent auditors) could even be given the capability to monitor the log (Trillian supports having auditors that check the log’s consistency[transparency.dev](https://transparency.dev/#:~:text=activity.%20It%20is%20an%20open,ledger%20based%20ecosystems%2C%20certificate%20transparency)).
- **Penetration Testing and Threat Modeling:** As features are added, continually perform threat modeling to identify any new attack surfaces. For example, introduction of ZKPs – ensure that an attacker can’t abuse the ZKP system to trick Pavilion. Or, when allowing more dynamic configuration, watch out for misconfigurations. Before going live, hire third-party security testers to pen-test the system, particularly the external API endpoints and the auth flows. Also test the resilience of the cryptographic implementations (timing attacks, etc.). Use of OPA means a miswritten policy could expose data, so have policies reviewed and tested. Incorporate these security checks into CI/CD if possible (there are OPA policy testing frameworks, static code analysis, etc.).

### **Compliance Across Regions – Example Considerations**

To illustrate concretely how we meet compliance in different regions, consider a few scenarios:

- **European Union (GDPR/EIDAS):** Pavilion operating in the EU will ensure GDPR compliance by *defaulting to privacy-preserving techniques*. Minimal personal data goes through the hub, and what does is encrypted and consented. Data residency: all EU personal data stays on EU servers. For eIDAS, if Pavilion is used to verify official credentials, it may need certification under the EU trust framework. Pavilion could choose to become an **eIDAS Qualified Trust Service** (depending on if its role fits any category like identity broker). That entails stringent security and audit requirements, which our architecture (with tamper-proof logs and strong auth) is aligning with. The use of BBS+ is actually pointed to by EU discussions as a way to achieve GDPR-compliant selective disclosure[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=To%20advance%20the%20Selective%20disclosure,ARF)[blog.worldline.tech](https://blog.worldline.tech/2024/05/14/bbs-plus-credentials.html#:~:text=citizens,BBS23), so Pavilion would be ahead by implementing that. We’d also need to handle user rights: e.g., if a user requests their data or deletion, Pavilion should be able to show what records it holds (likely just audit entries) and possibly delete or anonymize them if legitimate. However, because Pavilion holds so little (mostly logs), the burden is less – still, we should document this in privacy notices.
- **United States:** Privacy laws are more sectoral (HIPAA for health, GLBA for finance, etc.) and state-specific (California’s CCPA for example). Pavilion should adapt by configuring policies per sector and possibly deploying separate instances for highly sensitive sectors. For example, a Pavilion instance for healthcare might reside in a HIPAA-compliant cloud environment and enforce stricter access (only certified medical RPs, all data encrypted at rest with specific FIPS 140-2 validated modules, etc.). For consumer data (CCPA), Pavilion should support providing an accounting of data shared for a consumer if requested. Our audit log can serve as that accounting. If a California user asks “who did you share my data with via Pavilion?”, we can query audit logs by user (if we log a user identifier or transaction ID) and produce a report. This would fulfill CCPA’s requirement to disclose data sharing upon request. Additionally, in the US context, zero-knowledge proofs can help comply with “Minimize data collection” guidance, as they avoid collecting unnecessary data while still providing assurance (something regulators are increasingly appreciative of).
- **Other Regions:** Each region has its nuance (for instance, some countries require the software to be deployed in-country for government use, or encryption keys of a certain strength, etc.). Pavilion’s approach of modular services means we could accommodate custom requirements relatively easily. E.g., if a country disallows certain encryption algorithms, we can configure the crypto libraries accordingly for that region’s deployment. Or if data must be reviewed by a local authority, perhaps provide a special audit view. Importantly, being flexible and transparent wins points in compliance. Pavilion’s logging of every policy decision and data exchange means we can always explain and justify our operations.

### **DevOps, Automation, and Future Features**

As part of building out, we should also invest in good DevOps practices and plan for continuous improvement:

- **Infrastructure as Code:** Use Terraform or similar to manage cloud resources for Pavilion. This will allow consistent setup across dev, staging, prod, and across regions. It also helps in compliance (auditors can review IaC to see if it matches the intended architecture). Automated deployment pipelines (CI/CD) should be set up so that new versions of services can be rolled out with minimal downtime (Kubernetes and canary deployments can assist).
- **Testing:** Expand automated tests to include not just unit tests, but integration tests that simulate flows between RP and DP, and security tests (like ensure policies are correctly enforced). Possibly include **compliance tests** – e.g., a test that tries to violate a policy (simulate a rogue RP request) and assert that Pavilion blocks it and logs it. This ensures that as code changes, the core guarantees remain intact.
- **Global Directory & Onboarding:** As the network of RPs and DPs grows, Pavilion may need a user-friendly way to onboard new partners. In the future, a **Portal** could be provided where a new RP registers, uploads its public keys or OAuth client details, and subscribes to certain data products from DPs (with proper contracts in place). Similarly, DPs can publish what data/VCs they offer. Pavilion then loads this config (in Postgres, for example) and enforces it. Streamlining onboarding while maintaining security (maybe using workflows for approval) will be a feature to consider.
- **Interoperability:** Pavilion should stay updated with industry developments. For instance, the US and others exploring “portable identity” might have standards Pavilion can adopt. The mention of ORY (an alternative to Keycloak) suggests keeping an eye on evolving OAuth/OIDC solutions – perhaps migrating to newer standards like GNAP (if it gains traction) or integrating decentralized identity (DIDComm protocols) if relevant. The architecture should allow plugging in new components – e.g., swapping Keycloak for another IAM if needed, or adding support for DIDs in addition to OIDC.
- **Audit Log Scalability:** As usage grows, the audit log will become huge. We should plan how to manage it: QLDB, if used, can scale but has costs and currently (as of 2025) AWS stopped new signups – we might consider alternatives like **Amazon Timestream** (less verifiable though) or **Azure Confidential Ledger** or stick to **Trillian** self-hosted. If using Trillian, ensure the backing store (MySQL or etcd) is scalable, and consider sharding logs by region or year. Provide tools to query and extract portions of the log for audit without exposing everything (cryptographic proofs allow that). Possibly integrate the log with an analytics system (to detect trends, as mentioned). These are more maintenance tasks but important for long-term viability.

By the end of the build-out phase, Pavilion should be a **mature platform**: it will have the technical robustness (scalability, monitoring, failover) and the trust guarantees (security, privacy, compliance evidence) required for real-world adoption.

## **Conclusion**

In summary, **Phase 1** produces a tangible prototype of Pavilion – a mini deployment on a local machine with containerized services demonstrating how an RP can verify a user’s credential from a DP through the trust broker, using policy checks, privacy-preserving techniques, and audit logging. This MVP provides the foundation and confidence to proceed. Then, **Phase 2** focuses on expanding and hardening this into a production-ready service: scaling the architecture on Kubernetes with service mesh for secure communications, fully implementing advanced cryptographic features (VCs, ZKPs, PSI, accumulators) for maximal privacy, and embedding compliance measures across the design to meet regional data protection laws. The system will enforce strict policies via OPA, keep cryptographically verifiable audit trails of all activity, and provide comprehensive observability for operations.

Crucially, Pavilion’s design choices (minimal data storage, heavy use of encryption and proofs, fine-grained policy control) directly address compliance and trust – by design, it limits what data is handled and how, which makes legal compliance easier. In multiple regions with varying rules, Pavilion will deploy regionally and adhere to local requirements (data residency[ibm.com](https://www.ibm.com/think/insights/data-residency-why-is-it-important#:~:text=1,the%20data%20storage%20and%20processing), consent, etc.), essentially acting as a federated network of trust brokers that interoperate. By the end of these phases, we would have not just a functional product but one that is **resilient, scalable, and trustworthy** – capable of linking organizations and verifying credentials in a secure, privacy-preserving manner that respects users’ rights and organizations’ obligations.

The journey from prototype to production will involve iterative testing, feedback from pilot integrations, and possibly incremental additions (like new APIs, developer tools for RPs/DPs). However, with the robust stack and careful planning outlined, Pavilion is well-positioned to become a reliable infrastructure for digital trust in a compliant and future-proof way. All these steps ensure that when Pavilion is eventually launched as a live service, stakeholders can have confidence that it meets technical needs and the stringent **compliance needs across regions** – enabling global yet privacy-conscious verification of identities and credentials.